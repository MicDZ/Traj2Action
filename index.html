<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="VLA">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment</title>

  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet"> -->

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/dataTables.bulma.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="./static/js/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>
  <body>
 


  </div>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" ><span class="rainbow_text_animated">From Human Hands to Robot Arms</span>&nbsp;<img src="static/images/logo.png" width="60"><br> <div style="font-size:2rem"> Manipulation Skills Transfer via Trajectory Alignment</div></h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
            </div>

                  <!-- <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://anonymous.4open.science/r/Traj2Action-4A45/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Hugging face link -->
                 <!-- <span class="link-block">
                  <a href="/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                     <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>  -->

                 <video controls autoplay muted playsinline loop style="width:90%;max-width: 900px; height:auto;display:block;border-radius:6px; margin-bottom: 20px; margin-top: 20px;" poster="static/images/main.png">
        <source src="static/videos/teaser.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>

 
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div style="display: flex; justify-content: center;">
          <h2 class="subtitle has-text-justified" style="max-width: 800px; align-self: center;">
            <p>
              Learning diverse manipulation skills for real-world robots is severely bottlenecked by the reliance on costly and hard-to-scale teleoperated demonstrations. While human videos offer a scalable alternative, effectively transferring manipulation knowledge is fundamentally hindered by the significant morphological gap between human and robotic embodiments. To address this challenge and facilitate skill transfer from human to robot, we introduce <b>Traj2Action</b>,a novel framework that bridges this embodiment gap by using the 3D trajectory of the operational endpoint as a unified intermediate representation, and then transfers the manipulation knowledge embedded in this trajectory to the robot's actions. Our policy first learns to generate a coarse trajectory, which forms an high-level motion plan by leveraging both human and robot data. This plan then conditions the synthesis of precise, robot-specific actions (e.g., orientation and gripper state) within a co-denoising framework. Extensive real-world experiments on a Franka robot demonstrate that Traj2Action boosts the performance by up to <b>27%</b> and <b>22.25%</b> over 
         baseline on short- and long-horizon real-world tasks, and achieves significant gains as human data scales in robot policy learning.
            </p>
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
        <img src="static/images/main.png" height="100%" style="margin-bottom: 20px; margin-top: 20px;">
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered" style="margin-top: 20px;">Learning to Act by Watching Humans</h2>
      <br>
      <h2 class="subtitle has-text-justified">
       
Why can a person learn to stack cups in seconds, while training a robot to do the same can take days of repetitive trial and error? The secret lies in the data. Robots traditionally learn from their own, often slow and costly, experiences. This data bottleneck is one of the biggest challenges in creating versatile and capable machines.
      </h2>
           <h2 class="subtitle has-text-justified">
 
Our work explores a simple yet powerful idea: What if robots could learn complex tasks simply by watching people?
           </h2>
                 <h2 class="subtitle has-text-justified">

We present a novel learning framework that combines a small amount of direct robot experience with a large, scalable dataset of human hand demonstrations captured by multiple cameras. Our model learns to understand the underlying intent and long-horizon strategy from human movement, and then translates this knowledge into precise actions for its own robotic body.
                 </h2>
                       <h2 class="subtitle has-text-justified">
The result is a more data-efficient training process that produces more robust and successful robot policies. Dive in to see our method in action and explore the future of scalable robot learning.</h2>

    </div>
  </div>

</section> -->

<section class="hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Tasks Description</h2>
   <h2 >
      <div class="columns is-multiline is-variable is-4">
        <div class="column is-half">
          <div class="box task-box">
            <video controls autoplay muted playsinline loop style="width:100%;height:auto;display:block;border-radius:6px" poster="static/images/main.png">
              <source src="static/videos/tasks/task1-bottle.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p class="has-text-centered has-text-weight-semibold task-desc" style="margin-top: 8px;">Task 1: pick up the water bottle</p>
            <div class="task-hover-card">
              <p class="title is-6">Task 1: pick up the water bottle</p>
              <p class="is-size-6">The robot arm is tasked with locating a water bottle placed on a tabletop, moving towards it, and grasping it successfully. This task evaluates the model's fundamental pick-and-place capabilities.</p>
            </div>
          </div>
        </div>
        <div class="column is-half">
          <div class="box task-box" id="task2-box" data-yellow-src="static/videos/tasks/task2-fruits-yellow.mp4" data-blue-src="static/videos/tasks/task2-fruits-blue.mp4">
            <div class="task2-toggle" aria-label="Tray toggle" style="position:absolute; top:8px; right:8px; z-index:4; display:flex; gap:6px;">
              <button class="button is-small is-light" data-tray="yellow">Yellow</button>
              <button class="button is-small is-link is-light" data-tray="blue">Blue</button>
            </div>
            <video id="task2-video" src="static/videos/tasks/task2-fruits-yellow.mp4" controls autoplay muted playsinline loop style="width:100%;height:auto;display:block;border-radius:6px" poster="static/images/main.png">
              Your browser does not support the video tag.
            </video>
            <p class="has-text-centered has-text-weight-semibold task-desc" id="task2-title" style="margin-top: 8px;">Task 2: pick up the tomato and put it in the yellow tray</p>
            <div class="task-hover-card" id="task2-hover">
              <p class="title is-6">Task 2: pick up the tomato and put it in the <span id="task2-tray-label">yellow</span> tray</p>
              <p class="is-size-6">The workspace contains a tomato and two trays, one yellow and one blue. The robot must pick up the tomato and place it into the tray specified by a language command (e.g., "the <span id='task2-tray-label-2'>yellow</span> tray"). This task tests the policy's ability to ground language instructions to specific objects and goals.</p>
            </div>
          </div>
        </div>
        <div class="column is-half">
          <div class="box task-box">
            <video controls autoplay muted playsinline loop style="width:100%;height:auto;display:block;border-radius:6px" poster="static/images/main.png">
              <source src="static/videos/tasks/task3-rings.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p class="has-text-centered has-text-weight-semibold task-desc" style="margin-top: 8px;">Task 3: stack the rings on the pillar</p>
            <div class="task-hover-card">
              <p class="title is-6">Task 3: stack the rings on the pillar</p>
              <p class="is-size-6">The scene includes a pillar (composed of a yellow column and a blue base), a yellow ring, and a red ring. The robot needs to pick up both rings, one by one, and place them onto the pillar. This task assesses multi-step object manipulation and precision.</p>
            </div>
          </div>
        </div>
        <div class="column is-half">
          <div class="box task-box">
            <video controls autoplay muted playsinline loop style="width:100%;height:auto;display:block;border-radius:6px" poster="static/images/main.png">
              <source src="static/videos/tasks/task4-cups.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <p class="has-text-centered has-text-weight-semibold task-desc" style="margin-top: 8px;">Task 4: stack the paper cups</p>
            <div class="task-hover-card">
              <p class="title is-6">Task 4: stack the paper cups</p>
              <p class="is-size-6">Three paper cups are placed on the table. The robot is required to stack them sequentially to form a single tower. This task evaluates the policy's ability to handle deformable objects and perform iterative, precise placement.</p>
            </div>
          </div>
        </div>
      </div>
    </h2>
      <h2 class="subtitle has-text-justified">
        We designed four distinct tasks to assess the capabilities of our policy in terms of basic manipulation, understanding of instructions, handling of multi-step logic, and precise object placement. Hover over each task video to see a detailed description.
      </h2>

      
      </div>

    </div>
  </div>
</section>


<section class="section hero  ">
  <div class="section tasks-container" >
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered" id="results">Results</h2>
  <table>
<thead>
  <tr>
    <th rowspan="2" class="left-align">Model Variants</th>
    <th colspan="2">      
      <span class="tooltip">
        SH (SR %)
        <span class="tooltiptext">Short-Horizon (Success Rate)</span>
      </span>
    </th>
    <th colspan="2">
      <span class="tooltip">
        LH (TP %)
        <span class="tooltiptext">Long-Horizon (Task Progress)</span>
      </span>
    </th>
    <th colspan="2">Avg. Improvement</th>
  </tr>
  <tr>
    <th>
      <span class="tooltip">
        PWB
        <span class="tooltiptext">pick up the water bottle</span>
      </span>
    </th>
    <th>
      <span class="tooltip">
        PTT
        <span class="tooltiptext">pick up the tomato and put it in the tray</span>
      </span>
    </th>
    <th>
      <span class="tooltip">
        SRP
        <span class="tooltiptext">stack the rings on the pillar</span>
      </span>
    </th>
    <th>
      <span class="tooltip">
        SPC
        <span class="tooltiptext">stack the paper cups</span>
      </span>
    </th>
    <th>
      <span class="tooltip">
        SH (SR %)
        <span class="tooltiptext">Short-Horizon (Success Rate)</span>
      </span>
    </th>
    <th>
      <span class="tooltip">
        LH (TP %)
        <span class="tooltiptext">Long-Horizon (Task Progress)</span>
      </span>
    </th>
  </tr>
</thead>
    <tbody>
      <tr>
        <td class="left-align">Baseline (π₀)</td>
        <td>48</td>
        <td>50</td>
        <td>23.75</td>
        <td>37.75</td>
        <td>—</td>
        <td>—</td>
      </tr>
      <tr>
        <td class="left-align">+ Trajectory Expert</td>
        <td>58</td>
        <td>60</td>
        <td>33.50</td>
        <td>54.25</td>
        <td>+10.00</td>
        <td>+13.13</td>
      </tr>
      <tr>
        <td class="left-align">+ Traj. Expert + Human Data</td>
        <td>76</td>
        <td>76</td>
        <td>44.75</td>
        <td>61.25</td>
        <td>+27.00</td>
        <td>+22.25</td>
      </tr>
    </tbody>
    <caption style="margin-bottom: 20px;">Performance comparison.</caption>
  </table>

  <h2 class="subtitle has-text-justified" style="margin-top: 1rem;">
    <b>Contribution of Trajectory Expert</b>&nbsp;
  Adding a trajectory expert boosts performance, especially on Long-Horizon tasks (e.g., SPC score: 37.75% → 54.25%, +16.50%). The expert generates a coarse spatio-temporal plan that simplifies low-level control. Without it, simply adding human data gives only minimal gains (52% vs. 50%), showing that the unified trajectory space is key to bridging the human–robot gap.
  </h2>
    <h2 class="subtitle has-text-justified" style="margin-top: 1rem;">
  <b>Contribution of Human Data</b>&nbsp;
  Integrating human demonstrations further improves results across tasks (e.g., +28% on PWB, +26% on PTT, +23.5% on SPC, +21% on SRP). Human data brings diverse motions that help the model handle difficult, long-horizon tasks, leading to more robust, generalizable plans.
  </h2>
  <div class="columns is-centered" style="margin-top: 2rem; margin-bottom: 2rem;">
    <div class="column is-half has-text-centered">
      <img src="static/images/performance_hand_data_scale.jpg" alt="Image 1" style="max-width:100%; border-radius:8px;">
      <p class="is-size-6" style="margin-top: 0.5rem;"><i>Impact of Human Data Scale on Policy Performance.</i></p>
    </div>
    <div class="column is-half has-text-centered">
      <img src="static/images/fps.jpg" alt="Image 2" style="max-width:60%; border-radius:8px;">
      <p class="is-size-6" style="margin-top: 0.5rem;"><i>Ablation study on the effect of different trajectory sampling frequencies (FPS) on model performance.</i></p>
    </div>
  </div>
  <h2 class="subtitle has-text-justified" style="margin-top: 1rem;">
  <b>Impact of Human Data Scale</b>&nbsp;
Performance improves steadily with more human demonstrations. For example, success on pick up the tomato and put it in the tray rises from 68% (no human data) to 76% with 460 demos. On the harder stack the paper cups task, 264 demos lift performance from 37.75% to 57.50%, reaching 61.25% with all 460 demos. This confirms that larger human datasets significantly enhance robot learning.
</h2>
<table>
  <caption>
    Performance comparison for the <em>pick up the tomato and put it in the tray</em> task under different data collection strategies.
  </caption>
  <thead>
    <tr>
      <th style="text-align:left;">Strategy</th>
      <th>Robot Data<br/>(#/min)</th>
      <th>Human Data<br/>(#/min)</th>
      <th>Total Data Time<br/>(min)</th>
      <th>Performance<br/>SR(%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:left;">Baseline</td>
      <td>408 / 202.29</td>
      <td>—</td>
      <td>202.29</td>
      <td>50</td>
    </tr>

    <tr>
      <td style="text-align:left;">+ Trajectory Expert<br/>+ Robot Data-Only</td>
      <td>408 / 202.29</td>
      <td>0 / 0</td>
      <td>202.29</td>
      <td>60</td>
    </tr>

    <tr>
      <td rowspan="3" style="text-align:left;">+ Trajectory Expert<br/>+ Human &amp; Robot Data</td>
      <td rowspan="3">270 / 133.70</td>
      <td>120 / 14.25</td>
      <td>147.95</td>
      <td>58</td>
    </tr>
    <tr>
      <td>240 / 28.61</td>
      <td>162.31</td>
      <td>60</td>
    </tr>
    <tr style="border-bottom: 2px solid black;">
      <td>635 / 75.39</td>
      <td>209.09</td>
      <td>62</td>
    </tr>
  </tbody>
</table>

  <h2 class="subtitle has-text-justified" style="margin-top: 1rem;">
  <b>Human Data as a Substitute</b>&nbsp;
Human data can replace expensive robot data while maintaining or improving results. With 240 human demos plus fewer robot demos, performance matches robot-only training (60%) but cuts collection time by 20%. With 635 human demos, results surpass the baseline (62%). Even 120 demos achieve 58%, close to robot-only performance. Human data is thus scalable, cost-effective, and efficient for training.
  </h2>
  <h2 class="subtitle has-text-justified" style="margin-top: 1rem;">
  <b>Impact of Trajectory Sampling Frequency</b>&nbsp;
Aligning human and robot motion speeds is critical. Best results (76% success) come from sampling human trajectories at 30 FPS and robot ones at 10 FPS (a 3:1 ratio). This alignment prevents temporal mismatches and leads to more effective cross-embodiment learning.
  </h2>
  <h2 class="subtitle has-text-justified" style="margin-top: 1rem;">
  <b>Zero-Shot Generalization</b>&nbsp;
When trained only on placing tomatoes in a yellow tray, the policy still achieved 12% success when asked to use a blue tray — showing it can generalize to unseen tasks instead of just memorizing training data.
  </h2>
  </div>
  </div>
</section>

<!-- Comparison Section (Banner Carousel) -->
<section id="comparison" class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 id="evaluation_videos" class="title is-3 has-text-centered">
  Evaluation Videos
</h2>
      <h2 class="subtitle has-text-justified">
        We evaluate our method on a suite of four manipulation tasks, each designed to test different aspects of robotic skill transfer from human demonstrations. 
      </h2>
      <hr>
      <h2 class="title is-4 has-text-centered">pick up the water bottle</h2>


            <div class="comparison-banner">
        <button class="cmp-nav cmp-prev" id="btl-prev" aria-label="Previous video">
          <span class="icon"><i class="fas fa-chevron-left"></i></span>
        </button>
        
        <div class="cmp-stage">

          <div class="cmp-title" id="btl-task-title">pick up the water bottle</div>
          <video id="btl-video" playsinline muted loop controls poster="static/images/main.png"></video>
        </div>
        <button class="cmp-nav cmp-next" id="btl-next" aria-label="Next video">
          <span class="icon"><i class="fas fa-chevron-right"></i></span>
        </button>
        <div class="cmp-counter" id="btl-counter">1/1</div>
      </div>
      <p class="is-size-7 has-text-grey has-text-centered" style="margin-top: 0.5rem;">path: static/videos/comparison/bottle_compare_video/</p>
      <hr>

      <h2 class="title is-4 has-text-centered">pick up the tomato and put it in the yellow/blue tray</h2>
      <div class="field has-addons has-addons-centered" style="margin-bottom: 0.75rem;">
        <p class="control"><button id="cmp-yellow" class="button is-small is-link is-light" aria-pressed="true">Yellow</button></p>
        <p class="control"><button id="cmp-blue" class="button is-small is-light" aria-pressed="false">Blue</button></p>
      </div>

      <div class="comparison-banner">
        <button class="cmp-nav cmp-prev" id="cmp-prev" aria-label="Previous video">
          <span class="icon"><i class="fas fa-chevron-left"></i></span>
        </button>
        <div class="cmp-stage">
          <div class="cmp-title" id="cmp-task-title">pick up the tomato and put it in the yellow tray</div>
          <video id="cmp-video" playsinline muted loop controls poster="./static/images/main.png"></video>
        </div>
        <button class="cmp-nav cmp-next" id="cmp-next" aria-label="Next video">
          <span class="icon"><i class="fas fa-chevron-right"></i></span>
        </button>
        <div class="cmp-counter" id="cmp-counter">1/1</div>
      </div>
      <p class="is-size-7 has-text-grey has-text-centered" style="margin-top: 0.5rem;">path: static/videos/comparison/fruits_compare_480/<span id="cmp-current-label">yellow</span>/</p>
    </div>
  </div>
</section>

<!-- Bottle Comparison Section (Banner Carousel) -->
<section id="comparison-bottle" class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">

    </div>
  </div>
  
</section>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->

<!-- End teaser image -->



<!-- Leaderboard -->





<!-- Paper abstract -->
 

<!-- End paper abstract -->







<!-- Paper Analysis -->


<!-- Paper Qualitative -->


<!-- End image carousel -->


<!--BibTex citation -->

<!--End BibTex citation -->


  <footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            <!-- This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. -->
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
  
    <script src="./static/js/jquery-3.3.1.min.js"></script>
    <script src="./static/js/bootstrap.bundle.min.js"></script>
    <script src="./static/js/jquery.csv.min.js"></script>
    <script src="./static/js/dataTables.min.js"></script>
    <script src="./static/js/dataTables.bulma.min.js"></script>
    <script src="./static/js/csv_to_html_table.js"></script>

  
  </body>
  </html>
